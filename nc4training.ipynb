{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING JUPYTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator import generate_conv\n",
    "from preprocess import DictPrepross, Zero_One, Normalizer\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "datafolder ='Data'\n",
    "fin = Dataset(os.path.join(datafolder, 'f522_dh.trainingdata_in.lcv.20190401_0000z.nc4'))\n",
    "fout = Dataset(os.path.join(datafolder, 'f522_dh.trainingdata_out.lcv.20190401_0000z.nc4'))\n",
    "x = fin.variables\n",
    "y = fout.variables\n",
    "n,p = x['Xdim'].shape[0],x['Ydim'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # size of batch\n",
    "header = list(x.keys())[6:]  # variables are relevant after \"time\"\n",
    "kernels = [] # list of kernels\n",
    "testprop = 0.9 # proportion of test data\n",
    "seed = 0;  np.random.seed(seed); # setting the seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Creation of the preprocessor to normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "header_dict = [\"ql\",\"qi\",\"q\",\"rl\",\"ri\",\"ts\",\"t\", \"pl\",\"emis\",\"o3\"]  # variable we preprocess\n",
    "fct = [] # how to preprocess them\n",
    "for i in range(5):\n",
    "    fct.append(Zero_One())\n",
    "for j in range(5):\n",
    "    fct.append(Normalizer())\n",
    "    \n",
    "D = DictPrepross(header_dict, fct)\n",
    "del(fct)\n",
    "D.fitonNetCDF(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fix parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = len(header) + np.sum(np.array( [len(k.header) for k in kernels]  ))\n",
    "batch_per_epoch = int(n*p / batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) ARCHITECTURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidir RNN followed by fcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.24.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 72, 256)           143360    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 72, 50)            12850     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 72, 50)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 72, 20)            1020      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 72, 20)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 72, 1)             21        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 73)                5329      \n",
      "=================================================================\n",
      "Total params: 162,580\n",
      "Trainable params: 162,580\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, RNN, SimpleRNN, GRU, Activation, Flatten\n",
    "from keras import optimizers\n",
    "from keras.layers import Bidirectional\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "modelbd = Sequential()\n",
    "modelbd.add(Bidirectional(LSTM(128, return_sequences=True),input_shape=(72, 11)))\n",
    "modelbd.add(Dense(50))\n",
    "modelbd.add(Activation('relu'))\n",
    "modelbd.add(Dense(20))\n",
    "modelbd.add(Activation('relu'))\n",
    "modelbd.add(Dense(1))\n",
    "modelbd.add(Flatten())\n",
    "modelbd.add(Dense(73))\n",
    "\n",
    "rmsprop = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=1.e-5)\n",
    "modelbd.compile(loss='mse', optimizer=rmsprop)\n",
    "modelbd.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator =  lambda Params, batch_size, nb=-1 : generate_conv( \\\n",
    "                            Params[0], Params[1], kernels=kernels,\\\n",
    "                            train_prop=testprop, header=header,\\\n",
    "                            batch_size = batch_size, maxbatch=nb,  preprocess=D, test_data=False)\n",
    "\n",
    "test_generator = lambda Params, batch_size, nb=1 : generate_conv( \\\n",
    "                            Params[0], Params[1], kernels=kernels,\\\n",
    "                            train_prop=testprop, header=header,\\\n",
    "                            batch_size = batch_size, maxbatch=nb, preprocess=D, test_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsize = 128*4\n",
    "for x0,y0 in test_generator( (x,y), batch_size=testsize, nb=1):\n",
    "    data_test = (x0,y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if len(K.tensorflow_backend._get_available_gpus())>0:\n",
    "    history = modelbd.fit_generator( train_generator( (x, y), batch_size=64),steps_per_epoch=50, epochs=10, validation_data=data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "model_folder = \"TrainedModels\"\n",
    "prefix = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "archi = \"Bidir\"\n",
    "if not os.path.isdir(model_folder):\n",
    "    os.mkdir(model_folder)\n",
    "modelbd.save(  os.path.join(model_folder, prefix+archi+'.h5')  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VISUALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190501152954Bidir.h5 40.024800618489586\n",
      "20190501155636Bidir.h5 35.029927571614586\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for weight in os.listdir(\"TrainedModels/\"):\n",
    "    models.append(load_model(os.path.join(\"TrainedModels\",weight)))\n",
    "    ytruth = data_test[1]\n",
    "    ypred  = models[-1].predict(data_test[0])\n",
    "    print(weight, np.sum(np.abs(ypred-ytruth))/testsize/72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def multipred(models,x):\n",
    "    bs = x.shape[0]\n",
    "    yp = np.zeros((len(models),bs,73))\n",
    "    for i,model in enumerate(models):\n",
    "        yp[i,:,:]=model.predict(x)\n",
    "    return(yp)\n",
    "\n",
    "ytruth = data_test[1]\n",
    "ypred  = multipred(models, data_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=np.random.randint(testsize)\n",
    "plt.plot(ypred[0,i],np.arange(73)); plt.legend(\"pred1\");\n",
    "plt.plot(ypred[1,i],np.arange(73)); plt.legend(\"pred2\");\n",
    "plt.plot( (ypred[1,i]+ypred[0,i])/2,np.arange(73)); plt.legend(\"pred1\");\n",
    "plt.plot(ytruth[i],np.arange(73)); plt.legend(\"truth\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECOND ARCHITECTURE :\n",
    "Not tested yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateConv1D(n_unit, kernel_size, reg=0, seed=0, inputsize=False):\n",
    "    if not inputsize:\n",
    "        return(Conv1D(filters=n_unit, kernel_size=kernel_size, \\\n",
    "                 kernel_regularizer=regularizers.l2(reg), \\\n",
    "                 padding=\"same\", kernel_initializer = initializers.normal(seed=seed)))\n",
    "    else:\n",
    "        return(Conv1D(input_shape=inputsize, filters=n_unit, kernel_size=kernel_size, \\\n",
    "                 kernel_regularizer=regularizers.l2(reg), \\\n",
    "                 padding=\"same\", kernel_initializer = initializers.normal(seed=seed)))\n",
    "        \n",
    "\n",
    "def CreateModel(Hn, Ks, inputsize, reg=0, seed=0):\n",
    "    model = Sequential()\n",
    "    model.add(CreateConv1D(Hn[0], Ks[0], reg, seed, inputsize))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    for i in range(1,len(Hn)-1):\n",
    "        model.add(GenerateConv1D(Hn[i], Ks[i], reg, seed, False))\n",
    "        model.add(Activation(\"relu\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(73, kernel_regularizer = regularizers.l2(0.01),\\\n",
    "                kernel_initializer = initializers.normal(seed=s)))\n",
    "    return(model)\n",
    "\n",
    "HNn = [128,64,16,4] # hidden layer size\n",
    "Ks = [10, 10, 3, 3]\n",
    "inputsize = (lev, d)\n",
    "\n",
    "model = CreateModel(HNn, Ks, inputsize, 0.01)\n",
    "\n",
    "sgd = optimizers.SGD(lr=0., decay=1e-6, momentum=0.95)\n",
    "\n",
    "model.compile(optimizer=sgd,\n",
    "             loss='mse',\n",
    "             )\n",
    "model.save('arch1_FCNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.24.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Activation,Input,Flatten, UpSampling1D\n",
    "from keras import optimizers, regularizers, initializers\n",
    "\n",
    "# PARAMETERS :\n",
    "batch_size = 64\n",
    "header = list(x.keys())[6:] # variables are relevant after \"time\"\n",
    "kernels = []\n",
    "testprop = 0.9\n",
    "s = 0\n",
    "np.random.seed(s)\n",
    "\n",
    "n,p, lev = x['Xdim'].shape[0],x['Ydim'].shape[0],x['lev'].shape[0]\n",
    "\n",
    "d = len(header) + np.sum(np.array( [len(k.header) for k in kernels]  )).astype(int)\n",
    "batch_per_epoch = int(n*p / batch_size)\n",
    "batch_generator = lambda Params, batch_size : generate_conv( \\\n",
    "                            Params[0], Params[1], kernels=kernels,\\\n",
    "                            train_prop=testprop, header=header,\\\n",
    "                            batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III) TRACKS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 2 :\n",
    "\n",
    "- FCNN\n",
    "- (with AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MODEL 2 : FCM-Final FC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MODEL 2 : U-net :\n",
    "- use regular U-net so all layers affect each other and more stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 3 : Bidir-LSTM \n",
    "> Possible alternatives\n",
    "\n",
    "- use two LSTM to show both impact of superior and inferior layer\n",
    "- use attention model over it\n",
    "- use w embeddings before\n",
    "\n",
    "> TD\n",
    "\n",
    "- Read git trez\n",
    "- Read article of Hedge fun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
