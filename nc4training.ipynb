{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING JUPYTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! conda install -y tqdm\n",
    "data_folder=\"Data\"\n",
    "model_folder = \"TrainedModels\"\n",
    "log_folder = 'Log'\n",
    "\n",
    "##### Dictionnary\n",
    "D = Load_FLX_dict()\n",
    "##### Kernels\n",
    "Klist = []\n",
    "##### b_size\n",
    "batch_size= 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from preprocess import ProdKernel, FKernel, DictPrepross, Level_Normalizer\n",
    "from utils import Load_FLX_dict, Plot_Batch\n",
    "from generator import Diff_Generator, Up_and_Down_Generator\n",
    "\n",
    "##### Dictionnary\n",
    "D2 = []\n",
    "D2 = [ DictPrepross(['o3','pl'], [Level_Normalizer(False),Level_Normalizer(True)] )]\n",
    "D = [Load_FLX_dict()]\n",
    "##### Kernels\n",
    "Klist = []\n",
    "##### Full Preprocessing :\n",
    "FP = D + D2 + Klist\n",
    "##### b_size\n",
    "batch_size= 64\n",
    "\n",
    "train_generator =  Up_and_Down_Generator(folder=data_folder, batch_size=batch_size, train=True, preprocess_x=FP)\n",
    "validation_generator =  Up_and_Down_Generator(folder=data_folder, batch_size=batch_size, train=False, preprocess_x=FP, custom_b_p_e = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) ARCHITECTURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidir RNN followed by fcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Activation, Flatten, Input, TimeDistributed\n",
    "from keras.layers import Conv1D, UpSampling1D, AveragePooling1D, SeparableConv1D\n",
    "from keras import optimizers\n",
    "from keras.layers import Bidirectional\n",
    "from keras import backend as K\n",
    "from keras.losses import mean_squared_error\n",
    "import keras\n",
    "\n",
    "def one_loss(y_true, y_pred,i ):\n",
    "    E = mean_squared_error(y_true[:,:,0], y_pred[:,:,0])\n",
    "    return E\n",
    "\n",
    "def flxd_loss(y_true, y_pred):\n",
    "    E = mean_squared_error(y_true[:,:,0], y_pred[:,:,0])\n",
    "    return E\n",
    "\n",
    "def flxu_loss(y_true, y_pred):\n",
    "    E = mean_squared_error(y_true[:,:,1], y_pred[:,:,1])\n",
    "    return E\n",
    "\n",
    "def dfdts_loss(y_true, y_pred, coef=50):\n",
    "    E = mean_squared_error(coef*y_true[:,:,2], coef*y_pred[:,:,2])\n",
    "    return E\n",
    "\n",
    "def Total_loss(y_true, y_pred):\n",
    "    E = flxd_loss(y_true, y_pred)\n",
    "    E += flxu_loss(y_true, y_pred)\n",
    "    E += dfdts_loss(y_true, y_pred)\n",
    "    return(E)\n",
    "\n",
    "\n",
    "n_channel = len(train_generator.variables)\n",
    "o_channel = len(train_generator.new_variables_pred)\n",
    "\n",
    "modelbd = Sequential()\n",
    "modelbd.add(Bidirectional(LSTM(128, return_sequences=True, use_bias=False),input_shape=(72, n_channel)))\n",
    "modelbd.add(Conv1D(50, use_bias=False,kernel_size=8 ,padding='same'))\n",
    "modelbd.add(AveragePooling1D(7, padding='same', stride = 1 ))\n",
    "modelbd.add(Activation('relu'))\n",
    "modelbd.add(Conv1D(50, kernel_size=5 ,padding='same'))\n",
    "modelbd.add(AveragePooling1D(4, padding='same', stride = 1 ))\n",
    "modelbd.add(Activation('relu'))\n",
    "modelbd.add(Conv1D(20, kernel_size=3 ,padding='same'))\n",
    "modelbd.add(Activation('relu'))\n",
    "modelbd.add(TimeDistributed(Dense(o_channel)))\n",
    "#modelbd.add(Flatten())\n",
    "\n",
    "M = Sequential()\n",
    "M.add(UpSampling1D(5))\n",
    "M.add(AveragePooling1D(26, padding='same', stride=5 ))\n",
    "newInput = Input(shape=(72,11))\n",
    "newOutputs  = M(newInput)\n",
    "newOutputs2 = modelbd(newOutputs)\n",
    "modelbd2 = keras.Model(newInput, newOutputs2)\n",
    "rmsprop = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=1.e-5)\n",
    "modelbd2.compile(loss=Total_loss, optimizer=rmsprop,  metrics=[flxd_loss,flxu_loss, dfdts_loss])\n",
    "modelbd2.summary()\n",
    "\n",
    "modelbd.compile(loss='mse', optimizer=rmsprop)\n",
    "prefix = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "archi = \"Bidir\"\n",
    "modelbd.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def __init__(self, frequency=1000):\n",
    "        super(LossHistory, self).__init__()\n",
    "        self.frequency=frequency\n",
    "\n",
    "    @property\n",
    "    def loss_name(self):\n",
    "        return(['flxu_loss', 'flxd_loss', 'dfdts_loss', 'loss'])\n",
    "        \n",
    "    \"\"\"Save the history of the loss \"\"\"\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = dict()\n",
    "        for n in self.loss_name:\n",
    "            self.losses[n] = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        #print(logs['batch'])\n",
    "        if(batch%self.frequency==0):\n",
    "            for n in self.loss_name:\n",
    "                self.losses[n].append( logs.get(n))\n",
    "  \n",
    "    def on_train_end(self, logs={}):\n",
    "        for n in self.loss_name:\n",
    "            self.losses[n] = np.array(self.losses[n])\n",
    "\n",
    "LH = LossHistory(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = modelbd2.fit_generator(generator=train_generator ,\n",
    "                    validation_data=validation_generator,\n",
    "                             shuffle=False,\n",
    "                               callbacks = [LH],\n",
    "                               epochs=1,\n",
    "                               verbose=1)\n",
    "\n",
    "modelbd.save(  os.path.join(model_folder, prefix+archi+'.h5')  )\n",
    "\n",
    "if(True):\n",
    "    from contextlib import redirect_stdout\n",
    "    with open(os.path.join(log_folder, prefix), 'w') as f:\n",
    "        with redirect_stdout(f):\n",
    "            modelbd2.summary()\n",
    "            modelbd.summary()\n",
    "            for i in FP:\n",
    "                print(i)\n",
    "            print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VISUALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCT PLOT\n",
    "\n",
    "def Difference(y,y0):\n",
    "#    y_cumsum = np.cumsum(y[i])\n",
    "#    y0_cumsum = np.cumsum(y0)\n",
    "    return(  np.mean(np.square(y-y0)))\n",
    "\n",
    "def Compare(y,y0, i=0):\n",
    "    f=plt.figure( figsize=(15,8), dpi=80)\n",
    "    ax= f.add_subplot(1,2,1)\n",
    "    ax.plot(np.flip(y0[i]), np.arange(len(y0[i]))) \n",
    "    ax.plot(np.flip(y[i]), np.arange(len(y0[i]))) \n",
    "    ax.legend([\"y pred\", 'y truth'])\n",
    "#    ax.title(\"Diff\")\n",
    "    y_cumsum = np.cumsum(y[i])\n",
    "    y0_cumsum = np.cumsum(y0[i])\n",
    "    ax= f.add_subplot(1,2,2)\n",
    "    ax.plot(np.flip(y0_cumsum), np.arange(len(y0[i])))        \n",
    "    ax.plot(np.flip(y_cumsum), np.arange(len(y0[i])))        \n",
    "    ax.legend([\"y pred\", 'y truth'])\n",
    "#    ax.title(\"Cumulative\")\n",
    "    plt.show()\n",
    "\n",
    "def eliminate_var(m,x):\n",
    "    O = []\n",
    "    for i in range(11):\n",
    "        x0= x.copy()\n",
    "        x0*=0\n",
    "        x0[:,:,i]=x[:,:,i]\n",
    "        O.append(modelbd.predict(x0))\n",
    "    return(O)\n",
    "\n",
    "def Plot_Predictions(O, y, header):\n",
    "    f=plt.figure( figsize=(15,10), dpi=80)\n",
    "    for i,y0 in enumerate(O):\n",
    "        ax= f.add_subplot(3,4,i+1)\n",
    "        ax.set_title(header[i])\n",
    "        for b in range(y0.shape[0]):\n",
    "            ax.plot(np.flip(y0[b]), np.arange(len(y0[b])))\n",
    "    ax= f.add_subplot(3,4,12)\n",
    "    ax.set_title('flx')\n",
    "    for b in range(y0.shape[0]):\n",
    "        ax.plot(np.flip(y[b]), np.arange(len(y[b])))\n",
    "\n",
    "def Normal2(x,header):\n",
    "    O1 = []#['fcld', 'q','qi','ql','rl','ri']\n",
    "    N = [ 'pl']\n",
    "    STD = []\n",
    "    STD2 = []\n",
    "    for i, h in enumerate(header):\n",
    "        if h in O1:\n",
    "            x[:,:,i] = np.max(x[:,:,i], axis=1).reshape(x.shape[0],1)\n",
    "        if h in N:\n",
    "            #print(h, np.mean(x[:,:,i], axis=0)[32])\n",
    "            x[:,:,i] -= np.mean(x[:,:,i], axis=0)\n",
    "        if h in STD:\n",
    "            x[:,:,i] /= (x[:,-1,i]+0.000000001).reshape(-1,1)         \n",
    "        if h in STD2:\n",
    "            x[:,:,i] /= (x[:,0,i]+0.000000001).reshape(-1,1)         \n",
    "    return(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECOND ARCHITECTURE :\n",
    "Not tested yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III) TRACKS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 2 :\n",
    "\n",
    "- FCNN\n",
    "- (with AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MODEL 2 : FCM-Final FC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MODEL 2 : U-net :\n",
    "- use regular U-net so all layers affect each other and more stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 3 : Bidir-LSTM \n",
    "> Possible alternatives\n",
    "\n",
    "- use two LSTM to show both impact of superior and inferior layer\n",
    "- use attention model over it\n",
    "- use w embeddings before\n",
    "\n",
    "> TD\n",
    "\n",
    "- Read git trez\n",
    "- Read article of Hedge fun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
