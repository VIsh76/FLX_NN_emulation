{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "fin = Dataset('f522_dh.trainingdata_in.lcv.20190401_0000z.nc4') \n",
    "fout = Dataset('f522_dh.trainingdata_out.lcv.20190401_0000z.nc4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xdim (720,)\n",
      "Ydim (4320,)\n",
      "lons (4320, 720)\n",
      "lats (4320, 720)\n",
      "lev (72,)\n",
      "time (1,)\n",
      "emis (1, 4320, 720)\n",
      "fcld (1, 72, 4320, 720)\n",
      "o3 (1, 72, 4320, 720)\n",
      "pl (1, 72, 4320, 720)\n",
      "q (1, 72, 4320, 720)\n",
      "qi (1, 72, 4320, 720)\n",
      "ql (1, 72, 4320, 720)\n",
      "ri (1, 72, 4320, 720)\n",
      "rl (1, 72, 4320, 720)\n",
      "t (1, 72, 4320, 720)\n",
      "ts (1, 4320, 720)\n"
     ]
    }
   ],
   "source": [
    "x = fin.variables\n",
    "y = fout.variables\n",
    "for var in x.keys():\n",
    "    print(var, x[var][:].shape)\n",
    "    \n",
    "n   = x['Xdim'].shape[0]    \n",
    "p   = x['Ydim'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel(object):\n",
    "    def __init__(self, ids):\n",
    "        self.ids = ids\n",
    "        \n",
    "    def apply(self, x):\n",
    "        return(x)       \n",
    "\n",
    "class ProdKernel(Kernel):\n",
    "    def __init__(self, ids=[]):\n",
    "        Kernel.__init__(self,ids)\n",
    "        self.ids=ids\n",
    "\n",
    "    def apply(self,x0, xheader):\n",
    "        x=x0.copy()\n",
    "        for ids in self.ids:\n",
    "            id1=np.where(ids[0]==xheader)[0]\n",
    "            id2=np.where(ids[1]==xheader)[0]\n",
    "            k = x[:,:,id1]*x[:,:,id2]\n",
    "            hx.append(ids[0]+'*'+ids[1])\n",
    "            x = np.concatenate((k,x),axis=2)\n",
    "        return(x)\n",
    "    \n",
    "Pk = ProdKernel()# [(\"ts\",\"o3\"),(\"ts\",\"rl\") ] )\n",
    "#x0 = Pk.apply(X, np.array(hx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4320 720\n"
     ]
    }
   ],
   "source": [
    "print(p,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 72, 12) (16, 73)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate(x, y, kernel, batch_size=16):\n",
    "\n",
    "    n   = x['Xdim'].shape[0]\n",
    "    p   = x['Ydim'].shape[0]\n",
    "    lev = x['lev'].shape[0]\n",
    "    \n",
    "    xheader = []\n",
    "\n",
    "    id_batches = np.arange(batch_size)\n",
    "    while True: \n",
    "        \n",
    "        dataX = np.zeros((batch_size,lev,1))\n",
    "        idp = id_batches[0] // n\n",
    "        idn = id_batches % n\n",
    "        \n",
    "        while max(id_batches)<n*p:\n",
    "            \n",
    "            Y = y['flx'][0,:, idp, idn]\n",
    "            Y = Y.swapaxes(0,1)\n",
    "            \n",
    "            for k in x.keys():\n",
    "                if(len(x[k].shape)>2):\n",
    "                    xheader.append(k)\n",
    "                    if(len(x[k].shape)==4):\n",
    "                        a = x[k][:,:,idp,idn]\n",
    "                        a = a.reshape(1,lev, -1)\n",
    "                        a = a.swapaxes(0,2)\n",
    "                    elif(len(x[k].shape)==3):\n",
    "                        a = x[k][:,idp,idn]\n",
    "                        a = a.repeat(lev,1).reshape(1,-1 , lev)\n",
    "                        a = a.swapaxes(0,1)\n",
    "                        a = a.swapaxes(1,2)\n",
    "                    dataX = np.concatenate((dataX,a),axis=2)\n",
    "            yield dataX,Y\n",
    "            id_batches+=batch_size        \n",
    "\n",
    "from tqdm import tqdm\n",
    "batch_size=16\n",
    "generate_batch = lambda A,batch_size : generate(A[0],A[1],A[2],batch_size)\n",
    "for i in tqdm(range(1)):\n",
    "    for X,Y in generate_batch( (x,y,Pk),batch_size ):\n",
    "        print(X.shape,Y.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAN BE MODIFY :\n",
    "   - tride : check\n",
    "   - padding : change value of padding (same for ex or lin)\n",
    "   - check act/kernel constraint (limit size of gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.24.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Activation,Input,Flatten\n",
    "from keras import optimizers, regularizers, initializers\n",
    "\n",
    "# PARAMETERS \n",
    "HNn = [128,64,16,4] # hidden layer size\n",
    "d=12 # number of variables ############\" CHANGE !!\n",
    "s=0\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(input_shape=(72,d),filters=HNn[0],kernel_regularizer=regularizers.l2(0.01), \\\n",
    "           kernel_size=10, strides=1, padding=\"same\", kernel_initializer = initializers.Zeros()),\n",
    "    Activation(\"relu\"),\n",
    "    Conv1D(filters=HNn[1], kernel_size=10, \\\n",
    "           kernel_regularizer=regularizers.l2(0.01), strides=1, padding=\"same\",kernel_initializer = initializers.RandomUniform(minval=-1, maxval=1, seed=s)),\n",
    "    Activation(\"relu\"),\n",
    "    Conv1D(filters=HNn[2], kernel_size=3, \\\n",
    "           kernel_regularizer=regularizers.l2(0.01), strides=1, padding=\"same\",kernel_initializer = initializers.RandomUniform(minval=-1, maxval=1, seed=s)),\n",
    "    Activation(\"relu\"),\n",
    "    Conv1D(filters=HNn[3], kernel_size=3, \\\n",
    "           kernel_regularizer=regularizers.l2(0.01), strides=1, padding=\"same\",kernel_initializer = initializers.RandomUniform(minval=-1, maxval=1, seed=s)),\n",
    "    Activation(\"relu\"),\n",
    "    Flatten(),\n",
    "    Dense(73, input_shape=(72*HNn[2],), kernel_regularizer = regularizers.l2(0.01),kernel_initializer = initializers.RandomUniform(minval=-1, maxval=1, seed=s))\n",
    "])\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9)\n",
    "model.compile(optimizer=sgd,\n",
    "             loss='mse',\n",
    "             )\n",
    "#model.summary()\n",
    "model.save('arch1_FCNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b38cd2ea61eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mbatch_generator\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'arch1_FCNN.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_generator' is not defined"
     ]
    }
   ],
   "source": [
    "# Training :\n",
    "\n",
    "model.fit_generator( batch_generator( (x, y, K) ,batch_size=16), steps_per_epoch=200, epochs=1)\n",
    "model.save('arch1_FCNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.get_weights()[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=model.weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MODEL 2 : FCM-Final FC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MODEL 1 : U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MODEL 1 : FCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 just fc :\n",
    "# 2 conv :\n",
    "# 3 read : AvgPool1D, Bayesian NN, Dropout, CNN on the sphere (spherical convolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(x, y, batch_size=16):\n",
    "\n",
    "    n   = x['Xdim'].shape[0]\n",
    "    p   = x['Ydim'].shape[0]\n",
    "    lev = x['lev'].shape[0]\n",
    "    idx = np.random.choice(np.arange(n*p), batch_size, replace=False)\n",
    "    \n",
    "    Y = y['flx'][:].reshape(1, lev, -1).swapaxes(0,2)[idx]\n",
    "    \n",
    "    databoard = np.zeros((batch_size,lev,1))\n",
    "    xheader = []\n",
    "\n",
    "    for k in x.keys():\n",
    "        a = x[k][:]\n",
    "        if(len(a.shape)>2):\n",
    "            xheader.append(k)\n",
    "            if(len(x[k].shape)==4):\n",
    "                a = a.reshape(1,lev, -1)\n",
    "                a = a.swapaxes(0,2)\n",
    "            elif(len(x[k].shape)==3):\n",
    "                a = a.reshape(1, -1)\n",
    "                a = a.repeat(lev,1).reshape(1,-1 , lev)\n",
    "                a = a.swapaxes(0,1)\n",
    "                a = a.swapaxes(1,2)\n",
    "            a = a[idx]\n",
    "            databoard = np.concatenate((databoard,a),axis=2)\n",
    "    return(databoard,xheader,Y)\n",
    "\n",
    "X,hx,Y = generate_batch(x,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
