{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING JUPYTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.24.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "# Imports et variables setting\n",
    "\n",
    "# General Libs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import keras\n",
    "import datetime\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "# Current files\n",
    "from generator import Up_and_Down_Generator, FC_Generator\n",
    "from preprocess import Level_Normalizer, DictPrepross\n",
    "\n",
    "from utils import Load_FLX_dict, Plot_Batch\n",
    "from CST import CST\n",
    "\n",
    "data_folder=  CST.Data_folder(CST)\n",
    "model_folder = CST.Model_folder(CST)\n",
    "log_folder = CST.Log_folder(CST)\n",
    "\n",
    "##### Dictionnary\n",
    "np.random.seed(0)\n",
    "D2 = []\n",
    "D2 = [DictPrepross(['o3','pl'], [Level_Normalizer(False),Level_Normalizer(False)] )]\n",
    "D = [Load_FLX_dict()]\n",
    "##### Kernels\n",
    "\n",
    "Klist = []\n",
    "fit_genator = Up_and_Down_Generator(folder=data_folder, batch_size=512, train=True, preprocess_x=D)\n",
    "D2[0]['pl'].fit(fit_genator[0][0][:,:,3])\n",
    "D2[0]['o3'].fit(fit_genator[0][0][:,:,2])\n",
    "\n",
    "##### Full Preprocessing :\n",
    "FP = D + D2 #+ Klist\n",
    "##### b_size\n",
    "batch_size= 64\n",
    "\n",
    "train_generator =  FC_Generator(folder=data_folder, batch_size=batch_size, train=True, preprocess_x=FP,custom_b_p_e = 50)\n",
    "validation_generator =  FC_Generator(folder=data_folder, batch_size=5*batch_size, train=False, preprocess_x=FP, custom_b_p_e = 50)\n",
    "\n",
    "header_x = train_generator.variables\n",
    "header_y = train_generator.new_variables_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) ARCHITECTURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BD or BD+Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'U_net'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f88401b291fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0marchitectures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mU_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLossHistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerate_Log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBidir_Causal_Conv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0marchitectures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflxd_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflxu_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfdts_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'U_net'"
     ]
    }
   ],
   "source": [
    "from architectures import U_net, LossHistory, Generate_Log, Bidir_Causal_Conv\n",
    "from architectures import total_loss, flxd_loss, flxu_loss, dfdts_loss\n",
    "\n",
    "from keras import optimizers\n",
    "import datetime\n",
    "\n",
    "ups=5\n",
    "pooling=22\n",
    "units = [300, 100,72*3]\n",
    "\n",
    "prefix = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "archi = \"FC\"\n",
    "\n",
    "#M_ffc = FFC_archi(ups=ups, pooling=pooling, BS=batch_size, list_of_unit=units )\n",
    "M = FC_archi(ups, pooling, units, batch_size, reg=0.0001, o_channel=3, lev= 72, unique_var = 3, level_var=8)\n",
    "\n",
    "LH = LossHistory()\n",
    "rmsprop = optimizers.RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=1.e-6)\n",
    "M.compile(loss=total_loss_fc, optimizer=rmsprop, metrics=[flxd_loss_fc,flxu_loss_fc, dfdts_loss_fc])\n",
    "M.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bidir_Casual_Conv2(list_of_kernel_s, list_of_filters, ups, pooling, in_channel, o_channel, lev=CST.lev(CST)):\n",
    "    \"\"\"\n",
    "    used as input for the Unet\n",
    "    \"\"\"\n",
    "    Input0 = Input(shape=(lev, in_channel), name=Name('Input',0), dtype='float32')\n",
    "\n",
    "    Flip_layer = lambda x: K.reverse(x, axes=0)\n",
    "    I_cp = UpSampling1D(ups, name=Name('Up',0))(Input0)\n",
    "    I_avg = AveragePooling1D(pooling, padding='same', stride=ups, name='AVG_p')(I_cp)\n",
    "    I_avg_flip = Lambda(Flip_layer, name=Name('Flip',0))(I_avg)\n",
    "\n",
    "    Conv1u = [I_avg]\n",
    "    Conv1d = [I_avg_flip]\n",
    "    ACT = []\n",
    "\n",
    "    # Normal\n",
    "    for i in range(len(list_of_filters[0])):\n",
    "        Conv1u.append(Conv1D(filters = list_of_filters[0][i], kernel_size= list_of_kernel_s[0][i], \\\n",
    "                        padding='causal', activation='relu', name=Name(\"Conv_u\",i+1), use_bias=True)(Conv1u[-1]))\n",
    "    # Flipped\n",
    "    for i in range(len(list_of_filters[1])):\n",
    "        Conv1d.append(Conv1D(filters = list_of_filters[1][i], kernel_size= list_of_kernel_s[1][i],\\\n",
    "                        padding='causal', activation='relu', name=Name(\"Conv_d\",i+1), use_bias=True)(Conv1d[-1]))\n",
    "\n",
    "    C_flip = Lambda(Flip_layer,name=Name('Flip',1))(Conv1d[-1])\n",
    "    C1d_prime = [Concatenate( name=Name('Concat',0))([Conv1u[-1], C_flip])]\n",
    "    for i in range(len(list_of_filters[2])):\n",
    "        if i==len(list_of_kernel_s)-1:\n",
    "            C1d_prime.append(Conv1D(filters = list_of_filters[2][i], kernel_size=list_of_kernel_s[2][i], \\\n",
    "                            padding='causal', name=Name(\"Conv_conc\",i), use_bias=False)(C1d_prime[-1]))\n",
    "        else:\n",
    "            C1d_prime.append(Conv1D(filters = list_of_filters[2][i], kernel_size=list_of_kernel_s[2][i], \\\n",
    "                            padding='causal', name=Name(\"Conv_conc\",i), use_bias=False, activation='relu')(C1d_prime[-1]))\n",
    "    return keras.Model(Input0, C1d_prime[-1])\n",
    "\n",
    "\n",
    "def Unet2(list_of_kernels_s, list_of_filters, list_of_pooling, Div=3, lev=CST.lev(CST), in_channel=11, o_channel=CST.output_y(CST)):\n",
    "    \"\"\"\n",
    "    Generate a Unet-Archictecture\n",
    "    list_of_kernels : list of 2 lists containing the kernel size for convolution\n",
    "    list_of_filters : list of 2 lists containing the number of filters for convolution\n",
    "    Div : number of downscaling\n",
    "    in_channel : number of inputs\n",
    "    \"\"\"\n",
    "    Concats_l = []\n",
    "    Upsamplings_l = []\n",
    "    Convs_l1 = []\n",
    "    Convs_l2 = []\n",
    "    Poolings_l = []\n",
    "# DownScaling\n",
    "    Convs_l1.append(Input(name = 'Origin_Input',  dtype='float32', shape=(lev, in_channel)))\n",
    "    for i in range(Div):\n",
    "        Poolings_l.append(AveragePooling1D(list_of_kernels_s[0][i]-1, padding='same', stride=2, name=Name('AVG', i+1))(Convs_l1[-1]))\n",
    "        Convs_l1.append(Conv1D(filters=list_of_filters[0][i], kernel_size=list_of_kernels_s[0][i], \\\n",
    "                                padding='same', activation='relu', name=Name('Conv1',i+1))( Poolings_l[-1] ))\n",
    "\n",
    "# Operation done on the small dimension : here fc\n",
    "    Convs_l2.append(Flatten()(Convs_l1[-1])  )\n",
    "    Convs_l2.append(Dense( int(lev/2**Div) * list_of_filters[1][0]  )(Convs_l2[-1])  )\n",
    "    Convs_l2.append(Reshape(name='Reshape',input_shape=Convs_l2[-1].shape ,\\\n",
    "                            target_shape=( int(lev/2**Div)  ,  list_of_filters[1][0] ))(Convs_l2[-1]))\n",
    "\n",
    "# Upsampling and concats\n",
    "    for i in range(Div):\n",
    "        Concats_l.append(Concatenate( name=Name('Concat',i+1) )([Convs_l2[-1], Convs_l1[-i-1]]))\n",
    "        Upsamplings_l.append(UpSampling1D(2, name=Name('Ups',i+1))(Concats_l[-1]))\n",
    "        Convs_l2.append(Conv1D(filters=list_of_filters[2][i], kernel_size=list_of_kernels_s[2][i], \\\n",
    "                                padding='same', activation='relu', name=Name('Conv2',i+1))( Upsamplings_l[-1] ))\n",
    "    Conv3 = Conv1D(filters=o_channel, kernel_size=1, padding='same', use_bias=False)(Convs_l2[-1])\n",
    "    return keras.Model(Convs_l1[0],Conv3)\n",
    "###### FC\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'M' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f3e8788da83b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'M' is not defined"
     ]
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50/50 [==============================] - 4s 73ms/step - loss: 973479.4465 - flxd_loss: 141.3803 - flxu_loss: 49.9840 - dfdts_loss: 973288.1416 - val_loss: 392.6856 - val_flxd_loss: 107.3487 - val_flxu_loss: 29.8650 - val_dfdts_loss: 255.4719\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 2s 32ms/step - loss: 396.4329 - flxd_loss: 107.8757 - flxu_loss: 30.7457 - dfdts_loss: 257.8115 - val_loss: 406.8044 - val_flxd_loss: 110.6561 - val_flxu_loss: 31.5005 - val_dfdts_loss: 264.6477\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 2s 33ms/step - loss: 408.4122 - flxd_loss: 87.3486 - flxu_loss: 28.9185 - dfdts_loss: 292.1451 - val_loss: 443.2729 - val_flxd_loss: 95.7110 - val_flxu_loss: 29.3179 - val_dfdts_loss: 318.2440\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 2s 31ms/step - loss: 475.7501 - flxd_loss: 98.0895 - flxu_loss: 29.3522 - dfdts_loss: 348.3084 - val_loss: 444.6700 - val_flxd_loss: 95.7504 - val_flxu_loss: 30.7909 - val_dfdts_loss: 318.1287\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 2s 32ms/step - loss: 492.9929 - flxd_loss: 86.0603 - flxu_loss: 28.0399 - dfdts_loss: 378.8927 - val_loss: 399.1587 - val_flxd_loss: 108.3534 - val_flxu_loss: 30.7800 - val_dfdts_loss: 260.0253\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 2s 36ms/step - loss: 465.6213 - flxd_loss: 92.1871 - flxu_loss: 27.6244 - dfdts_loss: 345.8098 - val_loss: 374.8695 - val_flxd_loss: 82.3489 - val_flxu_loss: 19.0592 - val_dfdts_loss: 273.4613\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 1s 29ms/step - loss: 467.1732 - flxd_loss: 93.5999 - flxu_loss: 28.7053 - dfdts_loss: 344.8679 - val_loss: 480.9947 - val_flxd_loss: 103.4743 - val_flxu_loss: 30.9448 - val_dfdts_loss: 346.5756\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 2s 36ms/step - loss: 373.8463 - flxd_loss: 90.6866 - flxu_loss: 30.9244 - dfdts_loss: 252.2354 - val_loss: 370.4878 - val_flxd_loss: 79.8913 - val_flxu_loss: 20.5764 - val_dfdts_loss: 270.0201\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 1s 28ms/step - loss: 464.7950 - flxd_loss: 91.9377 - flxu_loss: 28.6588 - dfdts_loss: 344.1985 - val_loss: 369.7488 - val_flxd_loss: 80.2078 - val_flxu_loss: 18.9587 - val_dfdts_loss: 270.5822\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 1s 30ms/step - loss: 467.5394 - flxd_loss: 93.7559 - flxu_loss: 27.5529 - dfdts_loss: 346.2306 - val_loss: 442.3513 - val_flxd_loss: 94.9035 - val_flxu_loss: 29.4880 - val_dfdts_loss: 317.9598\n",
      "Epoch 11/50\n"
     ]
    }
   ],
   "source": [
    "history = M_conv.fit_generator(generator=train_generator ,\n",
    "                    validation_data=validation_generator,\n",
    "                             shuffle=False,\n",
    "                               callbacks = [LH],\n",
    "                               epochs=50,\n",
    "                               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_conv.save(  os.path.join(model_folder, prefix+archi+'.h5')  )\n",
    "Generate_Log([M_ffc], history, LH, os.path.join(log_folder,prefix+archi+'.txt'), seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9362945208044642"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1. / (1. + 1.e-6 * 68040))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [179.31939270019532],\n",
       " 'val_flxd_loss': [84.54665435791016],\n",
       " 'val_flxu_loss': [23.22427272796631],\n",
       " 'val_dfdts_loss': [71.54846427917481],\n",
       " 'loss': [124.8488102722168],\n",
       " 'flxd_loss': [69.37535011291504],\n",
       " 'flxu_loss': [21.22322248458862],\n",
       " 'dfdts_loss': [34.25023921966553]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "with open(os.path.join(log_folder, prefix), 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        print('Seed {}'.format(seed))\n",
    "        modelbd2.summary()\n",
    "        modelbd.summary()\n",
    "        for loss in FP:\n",
    "            print(loss)\n",
    "        print(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VISUALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCT PLOT\n",
    "def Show_triple_diff(y,y0):\n",
    "    j = 50 #np.random.randint(0)\n",
    "    f = plt.figure( figsize=(15,8)   )\n",
    "    ax = plt.subplot(131)\n",
    "    lev= 35\n",
    "    plt.plot(np.flip(y[:,:,0].T[:lev,j]) , np.arange(lev));\n",
    "    plt.plot(np.flip(y0[:,:,0].T[:lev,j]) , np.arange(lev));\n",
    "    ax.legend([\"truth\", \"pred\"])\n",
    "    ax = plt.subplot(132)\n",
    "    plt.plot(np.flip(y[:,:,1].T[:,j]) , np.arange(72));\n",
    "    plt.plot(np.flip(y0[:,:,1].T[:,j]) , np.arange(72));\n",
    "    ax.legend([\"truth\", \"pred\"])\n",
    "    ax = plt.subplot(133)\n",
    "    plt.plot(np.flip(y[:,:,2].T[:,j]) , np.arange(72));\n",
    "    plt.plot(np.flip(y0[:,:,2].T[:,j]) , np.arange(72));\n",
    "    ax.legend([\"truth\", \"pred\"])\n",
    "\n",
    "def Difference(y,y0):\n",
    "#    y_cumsum = np.cumsum(y[i])\n",
    "#    y0_cumsum = np.cumsum(y0)\n",
    "    return(  np.mean(np.square(y-y0)))\n",
    "\n",
    "def Compare(y,y0, i=0):\n",
    "    f=plt.figure( figsize=(15,8), dpi=80)\n",
    "    ax= f.add_subplot(1,2,1)\n",
    "    ax.plot(np.flip(y0[i]), np.arange(len(y0[i]))) \n",
    "    ax.plot(np.flip(y[i]), np.arange(len(y0[i]))) \n",
    "    ax.legend([\"y pred\", 'y truth'])\n",
    "#    ax.title(\"Diff\")\n",
    "    y_cumsum = np.cumsum(y[i])\n",
    "    y0_cumsum = np.cumsum(y0[i])\n",
    "    ax= f.add_subplot(1,2,2)\n",
    "    ax.plot(np.flip(y0_cumsum), np.arange(len(y0[i])))        \n",
    "    ax.plot(np.flip(y_cumsum), np.arange(len(y0[i])))        \n",
    "    ax.legend([\"y pred\", 'y truth'])\n",
    "#    ax.title(\"Cumulative\")\n",
    "    plt.show()\n",
    "\n",
    "def eliminate_var(m,x):\n",
    "    O = []\n",
    "    for i in range(11):\n",
    "        x0= x.copy()\n",
    "        x0*=0\n",
    "        x0[:,:,i]=x[:,:,i]\n",
    "        O.append(modelbd.predict(x0))\n",
    "    return(O)\n",
    "\n",
    "def Plot_Predictions(O, y, header):\n",
    "    f=plt.figure( figsize=(15,10), dpi=80)\n",
    "    for i,y0 in enumerate(O):\n",
    "        ax= f.add_subplot(3,4,i+1)\n",
    "        ax.set_title(header[i])\n",
    "        for b in range(y0.shape[0]):\n",
    "            ax.plot(np.flip(y0[b]), np.arange(len(y0[b])))\n",
    "    ax= f.add_subplot(3,4,12)\n",
    "    ax.set_title('flx')\n",
    "    for b in range(y0.shape[0]):\n",
    "        ax.plot(np.flip(y[b]), np.arange(len(y[b])))\n",
    "\n",
    "def Normal2(x,header):\n",
    "    O1 = []#['fcld', 'q','qi','ql','rl','ri']\n",
    "    N = [ 'pl']\n",
    "    STD = []\n",
    "    STD2 = []\n",
    "    for i, h in enumerate(header):\n",
    "        if h in O1:\n",
    "            x[:,:,i] = np.max(x[:,:,i], axis=1).reshape(x.shape[0],1)\n",
    "        if h in N:\n",
    "            #print(h, np.mean(x[:,:,i], axis=0)[32])\n",
    "            x[:,:,i] -= np.mean(x[:,:,i], axis=0)\n",
    "        if h in STD:\n",
    "            x[:,:,i] /= (x[:,-1,i]+0.000000001).reshape(-1,1)         \n",
    "        if h in STD2:\n",
    "            x[:,:,i] /= (x[:,0,i]+0.000000001).reshape(-1,1)         \n",
    "    return(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECOND ARCHITECTURE :\n",
    "Not tested yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III) TRACKS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 2 :\n",
    "\n",
    "- FCNN\n",
    "- (with AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MODEL 2 : FCM-Final FC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MODEL 2 : U-net :\n",
    "- use regular U-net so all layers affect each other and more stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 3 : Bidir-LSTM \n",
    "> Possible alternatives\n",
    "\n",
    "- use two LSTM to show both impact of superior and inferior layer\n",
    "- use attention model over it\n",
    "- use w embeddings before\n",
    "\n",
    "> TD\n",
    "\n",
    "- Read git trez\n",
    "- Read article of Hedge fun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
